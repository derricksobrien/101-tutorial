{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/derricksobrien/101-tutorial/blob/master/Asynchronous_Data_Workflow_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ASYNC DATA WORKFLOW DEMO\n",
        "A simple demonstration of building a concurrent data processing workflow\n",
        "using Python's asyncio, illustrating I/O concurrency and task management.\n",
        "\n",
        "Principles Illustrated:\n",
        "1. Concurrency via asyncio.gather()\n",
        "2. Non-blocking I/O using 'await' with aiohttp (simulated via asyncio.sleep)\n",
        "3. Task monitoring using asyncio.Task and callbacks\n",
        "\"\"\"\n",
        "import asyncio\n",
        "import time\n",
        "from typing import Any, List, Dict\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "# List of \"tasks\" or \"jobs\" to be processed concurrently.\n",
        "WORKFLOW_JOBS = [\n",
        "    {\"id\": \"DB_Fetch_User_A\", \"duration\": 2.5, \"type\": \"I/O\"},\n",
        "    {\"id\": \"API_Call_Service_B\", \"duration\": 4.0, \"type\": \"I/O\"},\n",
        "    {\"id\": \"File_Read_Config\", \"duration\": 1.0, \"type\": \"I/O\"},\n",
        "    {\"id\": \"Validate_Data_C\", \"duration\": 3.2, \"type\": \"I/O\"},\n",
        "    {\"id\": \"Heavy_Calc_D\", \"duration\": 0.5, \"type\": \"CPU\"}, # Short CPU simulation\n",
        "]\n",
        "\n",
        "# --- 1. Task Completion Callback (Monitoring Stage) ---\n",
        "def task_completed_callback(task: asyncio.Task) -> None:\n",
        "    \"\"\"\n",
        "    This synchronous function runs immediately upon the completion of its attached task.\n",
        "    It simulates a monitoring or logging step, providing instant feedback.\n",
        "    \"\"\"\n",
        "    job_id = task.get_name() # Get the unique name we assigned to the task\n",
        "    current_time = time.time() - WORKFLOW_START_TIME\n",
        "\n",
        "    if task.cancelled():\n",
        "        print(f\"\\n[--- MONITOR ---] ‚ùå Task '{job_id}' was cancelled.\")\n",
        "    elif task.exception():\n",
        "        print(f\"\\n[--- MONITOR ---] ‚ö†Ô∏è Task '{job_id}' FAILED! Error: {task.exception()}\")\n",
        "    else:\n",
        "        # Task completed successfully. Print the result.\n",
        "        result = task.result()\n",
        "        print(f\"\\n[--- MONITOR ---] ‚úÖ Task '{job_id}' FINISHED at T+{current_time:.2f}s.\")\n",
        "        print(f\"                | Result: Successfully processed {result} bytes.\")\n",
        "\n",
        "# --- 2. Asynchronous Workflow Step (I/O Concurrency) ---\n",
        "async def execute_job(job: Dict[str, Any]) -> int:\n",
        "    \"\"\"\n",
        "    The coroutine that performs the actual job. This function uses 'await'\n",
        "    to yield control during simulated I/O-bound wait times.\n",
        "    \"\"\"\n",
        "    job_id = job[\"id\"]\n",
        "    duration = job[\"duration\"]\n",
        "    current_time = time.time() - WORKFLOW_START_TIME\n",
        "\n",
        "    print(f\"[{job_id}] ‚û°Ô∏è  INITIATED at T+{current_time:.2f}s. Expected duration: {duration:.1f}s.\")\n",
        "\n",
        "    # --- THE KEY TO ASYNCHRONOUS CONCURRENCY ---\n",
        "    # We use await asyncio.sleep(duration) to simulate a non-blocking I/O operation\n",
        "    # (e.g., waiting for an HTTP response, a database query, or a file read).\n",
        "    # When this coroutine hits 'await', it pauses and returns control to the Event Loop,\n",
        "    # allowing the loop to switch to other pending tasks immediately.\n",
        "    await asyncio.sleep(duration)\n",
        "    # The coroutine is resumed when the Event Loop determines the sleep is over.\n",
        "\n",
        "    # Simulate a small, successful processing result (e.g., number of bytes read)\n",
        "    processed_bytes = random.randint(1000, 10000)\n",
        "    print(f\"[{job_id}] ‚¨ÖÔ∏è  COMPLETED simulated processing internally.\")\n",
        "    return processed_bytes\n",
        "\n",
        "# --- 3. Main Workflow Orchestrator ---\n",
        "async def main_workflow() -> List[Any]:\n",
        "    \"\"\"\n",
        "    The primary coroutine that sets up and manages the concurrent execution.\n",
        "    \"\"\"\n",
        "    global WORKFLOW_START_TIME\n",
        "    WORKFLOW_START_TIME = time.time()\n",
        "\n",
        "    print(\"=========================================================================\")\n",
        "    print(f\"| WORKFLOW STARTING: Processing {len(WORKFLOW_JOBS)} Jobs Concurrently |\")\n",
        "    print(\"=========================================================================\")\n",
        "\n",
        "    # Create a list of coroutine objects from the job list\n",
        "    coroutine_objs = [execute_job(job) for job in WORKFLOW_JOBS]\n",
        "\n",
        "    # --- Task Creation & Monitoring ---\n",
        "    tasks = []\n",
        "    for job, coro in zip(WORKFLOW_JOBS, coroutine_objs):\n",
        "        # 1. Create an asyncio.Task from the coroutine\n",
        "        # Tasks are scheduled immediately by the Event Loop after creation.\n",
        "        task = asyncio.create_task(coro, name=job['id'])\n",
        "\n",
        "        # 2. Attach the monitoring callback\n",
        "        # This allows us to track completion status for each task independently.\n",
        "        task.add_done_callback(task_completed_callback)\n",
        "        tasks.append(task)\n",
        "\n",
        "    print(f\"\\n[ORCHESTRATOR] üèóÔ∏è  {len(tasks)} Tasks have been scheduled concurrently on the Event Loop.\")\n",
        "    print(\"[ORCHESTRATOR] ‚è≥ Now 'await'ing all tasks via asyncio.gather()...\")\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "\n",
        "    # --- Concurrency Execution and Waiting ---\n",
        "    # asyncio.gather() runs all tasks concurrently and waits for the results of ALL of them.\n",
        "    # The total execution time will be dominated by the longest-running task (API_Call_Service_B at 4.0s).\n",
        "    all_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    # --- Finalization ---\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - WORKFLOW_START_TIME\n",
        "\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "    print(f\"[ORCHESTRATOR] üèÅ All tasks finished.\")\n",
        "    print(f\"[ORCHESTRATOR] ‚è±Ô∏è  Total Workflow Time: {total_time:.2f} seconds\")\n",
        "\n",
        "    # Filter out exceptions for the final summary\n",
        "    successful_results = [r for r in all_results if not isinstance(r, Exception)]\n",
        "\n",
        "    return successful_results\n",
        "\n",
        "# Global variable to track start time for relative timing in callbacks\n",
        "WORKFLOW_START_TIME = 0.0\n",
        "\n",
        "# --- 4. Run the Workflow ---\n",
        "\n",
        "final_data = await main_workflow()\n",
        "\n",
        "print(\"\\n=========================================================================\")\n",
        "print(f\"| FINAL WORKFLOW SUMMARY: {len(final_data)} Successful Results |\")\n",
        "print(\"=========================================================================\")\n",
        "total_bytes = sum(final_data)\n",
        "for i, data in enumerate(final_data):\n",
        "    print(f\"Result {i+1}: Processed {data} bytes.\")\n",
        "print(f\"\\nTotal Processed Bytes: {total_bytes}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================\n",
            "| WORKFLOW STARTING: Processing 5 Jobs Concurrently |\n",
            "=========================================================================\n",
            "\n",
            "[ORCHESTRATOR] üèóÔ∏è  5 Tasks have been scheduled concurrently on the Event Loop.\n",
            "[ORCHESTRATOR] ‚è≥ Now 'await'ing all tasks via asyncio.gather()...\n",
            "-------------------------------------------------------------------------\n",
            "[DB_Fetch_User_A] ‚û°Ô∏è  INITIATED at T+0.00s. Expected duration: 2.5s.\n",
            "[API_Call_Service_B] ‚û°Ô∏è  INITIATED at T+0.00s. Expected duration: 4.0s.\n",
            "[File_Read_Config] ‚û°Ô∏è  INITIATED at T+0.00s. Expected duration: 1.0s.\n",
            "[Validate_Data_C] ‚û°Ô∏è  INITIATED at T+0.00s. Expected duration: 3.2s.\n",
            "[Heavy_Calc_D] ‚û°Ô∏è  INITIATED at T+0.00s. Expected duration: 0.5s.\n",
            "[Heavy_Calc_D] ‚¨ÖÔ∏è  COMPLETED simulated processing internally.\n",
            "\n",
            "[--- MONITOR ---] ‚úÖ Task 'Heavy_Calc_D' FINISHED at T+0.50s.\n",
            "                | Result: Successfully processed 1333 bytes.\n",
            "[File_Read_Config] ‚¨ÖÔ∏è  COMPLETED simulated processing internally.\n",
            "\n",
            "[--- MONITOR ---] ‚úÖ Task 'File_Read_Config' FINISHED at T+1.00s.\n",
            "                | Result: Successfully processed 2461 bytes.\n",
            "[DB_Fetch_User_A] ‚¨ÖÔ∏è  COMPLETED simulated processing internally.\n",
            "\n",
            "[--- MONITOR ---] ‚úÖ Task 'DB_Fetch_User_A' FINISHED at T+2.50s.\n",
            "                | Result: Successfully processed 2247 bytes.\n",
            "[Validate_Data_C] ‚¨ÖÔ∏è  COMPLETED simulated processing internally.\n",
            "\n",
            "[--- MONITOR ---] ‚úÖ Task 'Validate_Data_C' FINISHED at T+3.20s.\n",
            "                | Result: Successfully processed 5228 bytes.\n",
            "[API_Call_Service_B] ‚¨ÖÔ∏è  COMPLETED simulated processing internally.\n",
            "\n",
            "[--- MONITOR ---] ‚úÖ Task 'API_Call_Service_B' FINISHED at T+4.00s.\n",
            "                | Result: Successfully processed 9157 bytes.\n",
            "-------------------------------------------------------------------------\n",
            "[ORCHESTRATOR] üèÅ All tasks finished.\n",
            "[ORCHESTRATOR] ‚è±Ô∏è  Total Workflow Time: 4.00 seconds\n",
            "\n",
            "=========================================================================\n",
            "| FINAL WORKFLOW SUMMARY: 5 Successful Results |\n",
            "=========================================================================\n",
            "Result 1: Processed 2247 bytes.\n",
            "Result 2: Processed 9157 bytes.\n",
            "Result 3: Processed 2461 bytes.\n",
            "Result 4: Processed 5228 bytes.\n",
            "Result 5: Processed 1333 bytes.\n",
            "\n",
            "Total Processed Bytes: 20426\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s308gtYhXrxa",
        "outputId": "67667c51-b4be-412e-8c61-d3e18df99146"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}